{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Method which takes single sentence and converts to array of \n",
    "# words without stopwords\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Converts reviews to array of sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences: \n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, \\\n",
    "                                             remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads up csvs of training data\n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2a9a1e26bca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Parsing sentences from training set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreview_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Parsing sentences from unlabeled set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cbed60505a84>\u001b[0m in \u001b[0;36mreview_to_sentences\u001b[0;34m(review, tokenizer, remove_stopwords)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mraw_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m,\u001b[0m                                              \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-43d8594a4311>\u001b[0m in \u001b[0;36mreview_to_wordlist\u001b[0;34m(review, remove_stopwords)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# words without stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUILDER_FEATURES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mbuilder_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuilder_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 raise FeatureNotFound(\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/bs4/builder/__init__.pyc\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;31m# Eliminate any candidates that don't have this feature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     candidate_set = candidate_set.intersection(\n\u001b[0;32m---> 68\u001b[0;31m                         set(we_have_the_feature))\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# The only valid candidates are the ones in candidate_set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parses the reviews and adds them as sentences inot the training set.\n",
    "import unidecode\n",
    "\n",
    "sentences = []\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review.decode('utf-8'), tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review.decode('utf-8'), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"processed_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_sentences.pkl', 'rb') as f:\n",
    "    sentences2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vectorized_reviews.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trainDataVecs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectorized_reviews_answers.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trainOutputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vectorized_reviews.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "with open('vectorized_reviews_answers.pkl', 'rb') as f:\n",
    "    outputs = pickle.load(f)\n",
    "\n",
    "X = dataset[1500:]\n",
    "y = outputs[1500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                   level=logging.INFO)\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                   level=logging.INFO)\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-24 12:40:32,972 : INFO : collecting all words and their counts\n",
      "2018-10-24 12:40:32,973 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-24 12:40:33,092 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-24 12:40:33,215 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2018-10-24 12:40:33,331 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2018-10-24 12:40:33,451 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2018-10-24 12:40:33,556 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2018-10-24 12:40:33,673 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2018-10-24 12:40:33,793 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2018-10-24 12:40:33,917 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2018-10-24 12:40:34,027 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2018-10-24 12:40:34,162 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2018-10-24 12:40:34,290 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2018-10-24 12:40:34,413 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2018-10-24 12:40:34,532 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2018-10-24 12:40:34,645 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2018-10-24 12:40:34,766 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2018-10-24 12:40:34,888 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2018-10-24 12:40:35,007 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2018-10-24 12:40:35,132 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2018-10-24 12:40:35,267 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2018-10-24 12:40:35,390 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2018-10-24 12:40:35,505 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2018-10-24 12:40:35,618 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2018-10-24 12:40:35,718 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2018-10-24 12:40:35,836 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2018-10-24 12:40:35,950 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2018-10-24 12:40:36,060 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2018-10-24 12:40:36,161 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
      "2018-10-24 12:40:36,266 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
      "2018-10-24 12:40:36,361 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
      "2018-10-24 12:40:36,472 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
      "2018-10-24 12:40:36,585 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
      "2018-10-24 12:40:36,692 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
      "2018-10-24 12:40:36,807 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
      "2018-10-24 12:40:36,934 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
      "2018-10-24 12:40:37,070 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
      "2018-10-24 12:40:37,198 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types\n",
      "2018-10-24 12:40:37,341 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types\n",
      "2018-10-24 12:40:37,463 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types\n",
      "2018-10-24 12:40:37,578 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types\n",
      "2018-10-24 12:40:37,691 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types\n",
      "2018-10-24 12:40:37,830 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types\n",
      "2018-10-24 12:40:38,014 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types\n",
      "2018-10-24 12:40:38,191 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types\n",
      "2018-10-24 12:40:38,330 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types\n",
      "2018-10-24 12:40:38,457 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types\n",
      "2018-10-24 12:40:38,586 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types\n",
      "2018-10-24 12:40:38,687 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types\n",
      "2018-10-24 12:40:38,795 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types\n",
      "2018-10-24 12:40:38,923 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types\n",
      "2018-10-24 12:40:39,065 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types\n",
      "2018-10-24 12:40:39,209 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types\n",
      "2018-10-24 12:40:39,339 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types\n",
      "2018-10-24 12:40:39,473 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types\n",
      "2018-10-24 12:40:39,610 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types\n",
      "2018-10-24 12:40:39,743 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types\n",
      "2018-10-24 12:40:39,866 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types\n",
      "2018-10-24 12:40:39,980 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types\n",
      "2018-10-24 12:40:40,086 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types\n",
      "2018-10-24 12:40:40,197 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types\n",
      "2018-10-24 12:40:40,305 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types\n",
      "2018-10-24 12:40:40,406 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types\n",
      "2018-10-24 12:40:40,513 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types\n",
      "2018-10-24 12:40:40,631 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types\n",
      "2018-10-24 12:40:40,752 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types\n",
      "2018-10-24 12:40:40,865 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types\n",
      "2018-10-24 12:40:40,970 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types\n",
      "2018-10-24 12:40:41,090 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types\n",
      "2018-10-24 12:40:41,201 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types\n",
      "2018-10-24 12:40:41,304 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types\n",
      "2018-10-24 12:40:41,402 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types\n",
      "2018-10-24 12:40:41,497 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types\n",
      "2018-10-24 12:40:41,607 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types\n",
      "2018-10-24 12:40:41,734 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-24 12:40:41,861 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types\n",
      "2018-10-24 12:40:41,990 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types\n",
      "2018-10-24 12:40:42,111 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types\n",
      "2018-10-24 12:40:42,242 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types\n",
      "2018-10-24 12:40:42,374 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types\n",
      "2018-10-24 12:40:42,496 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types\n",
      "2018-10-24 12:40:42,564 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences\n",
      "2018-10-24 12:40:42,566 : INFO : Loading a fresh vocabulary\n",
      "2018-10-24 12:40:42,740 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2018-10-24 12:40:42,741 : INFO : effective_min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
      "2018-10-24 12:40:42,793 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2018-10-24 12:40:42,800 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-10-24 12:40:42,801 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
      "2018-10-24 12:40:42,863 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2018-10-24 12:40:42,865 : INFO : resetting layer weights\n",
      "2018-10-24 12:40:43,079 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-10-24 12:40:44,088 : INFO : EPOCH 1 - PROGRESS: at 6.68% examples, 846694 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:40:45,101 : INFO : EPOCH 1 - PROGRESS: at 13.14% examples, 826276 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:40:46,106 : INFO : EPOCH 1 - PROGRESS: at 19.49% examples, 816678 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:40:47,108 : INFO : EPOCH 1 - PROGRESS: at 25.86% examples, 813837 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:40:48,111 : INFO : EPOCH 1 - PROGRESS: at 31.85% examples, 802192 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:40:49,132 : INFO : EPOCH 1 - PROGRESS: at 38.01% examples, 796614 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:40:50,139 : INFO : EPOCH 1 - PROGRESS: at 43.46% examples, 782356 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:40:51,139 : INFO : EPOCH 1 - PROGRESS: at 49.55% examples, 781994 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:40:52,147 : INFO : EPOCH 1 - PROGRESS: at 55.26% examples, 775545 words/s, in_qsize 6, out_qsize 0\n",
      "2018-10-24 12:40:53,150 : INFO : EPOCH 1 - PROGRESS: at 61.13% examples, 773475 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:40:54,156 : INFO : EPOCH 1 - PROGRESS: at 67.81% examples, 780420 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:40:55,161 : INFO : EPOCH 1 - PROGRESS: at 74.51% examples, 786173 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:40:56,174 : INFO : EPOCH 1 - PROGRESS: at 81.26% examples, 790893 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:40:57,178 : INFO : EPOCH 1 - PROGRESS: at 87.85% examples, 794424 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:40:58,184 : INFO : EPOCH 1 - PROGRESS: at 94.54% examples, 797928 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:40:58,994 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-24 12:40:59,000 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-24 12:40:59,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-24 12:40:59,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-24 12:40:59,011 : INFO : EPOCH - 1 : training on 17798082 raw words (12750107 effective words) took 15.9s, 800471 effective words/s\n",
      "2018-10-24 12:41:00,022 : INFO : EPOCH 2 - PROGRESS: at 6.23% examples, 789380 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:01,025 : INFO : EPOCH 2 - PROGRESS: at 12.87% examples, 811677 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:02,029 : INFO : EPOCH 2 - PROGRESS: at 19.60% examples, 823957 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:03,030 : INFO : EPOCH 2 - PROGRESS: at 26.25% examples, 828909 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:04,043 : INFO : EPOCH 2 - PROGRESS: at 32.86% examples, 828095 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:05,045 : INFO : EPOCH 2 - PROGRESS: at 39.48% examples, 830422 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:06,069 : INFO : EPOCH 2 - PROGRESS: at 46.22% examples, 832561 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:07,083 : INFO : EPOCH 2 - PROGRESS: at 52.83% examples, 832507 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:08,092 : INFO : EPOCH 2 - PROGRESS: at 59.39% examples, 833685 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:09,095 : INFO : EPOCH 2 - PROGRESS: at 66.06% examples, 835144 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:10,104 : INFO : EPOCH 2 - PROGRESS: at 72.72% examples, 836322 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:11,100 : INFO : EPOCH 2 - PROGRESS: at 79.17% examples, 835157 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:12,105 : INFO : EPOCH 2 - PROGRESS: at 85.63% examples, 833856 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:13,106 : INFO : EPOCH 2 - PROGRESS: at 92.04% examples, 833006 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:14,118 : INFO : EPOCH 2 - PROGRESS: at 98.29% examples, 829782 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:14,370 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-24 12:41:14,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-24 12:41:14,385 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-24 12:41:14,404 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-24 12:41:14,406 : INFO : EPOCH - 2 : training on 17798082 raw words (12752290 effective words) took 15.4s, 828625 effective words/s\n",
      "2018-10-24 12:41:15,420 : INFO : EPOCH 3 - PROGRESS: at 6.35% examples, 802410 words/s, in_qsize 7, out_qsize 1\n",
      "2018-10-24 12:41:16,423 : INFO : EPOCH 3 - PROGRESS: at 12.74% examples, 804292 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:17,423 : INFO : EPOCH 3 - PROGRESS: at 19.04% examples, 800581 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:18,429 : INFO : EPOCH 3 - PROGRESS: at 23.76% examples, 749625 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:19,434 : INFO : EPOCH 3 - PROGRESS: at 28.43% examples, 717527 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:20,447 : INFO : EPOCH 3 - PROGRESS: at 33.32% examples, 699960 words/s, in_qsize 7, out_qsize 1\n",
      "2018-10-24 12:41:21,438 : INFO : EPOCH 3 - PROGRESS: at 37.12% examples, 669426 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:22,442 : INFO : EPOCH 3 - PROGRESS: at 42.02% examples, 664111 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:23,457 : INFO : EPOCH 3 - PROGRESS: at 47.73% examples, 670209 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:24,470 : INFO : EPOCH 3 - PROGRESS: at 51.42% examples, 649699 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:25,477 : INFO : EPOCH 3 - PROGRESS: at 56.55% examples, 649946 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:26,478 : INFO : EPOCH 3 - PROGRESS: at 62.94% examples, 664216 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:27,486 : INFO : EPOCH 3 - PROGRESS: at 69.48% examples, 677094 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:28,484 : INFO : EPOCH 3 - PROGRESS: at 76.22% examples, 689942 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:29,502 : INFO : EPOCH 3 - PROGRESS: at 82.38% examples, 695551 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:30,510 : INFO : EPOCH 3 - PROGRESS: at 88.81% examples, 703418 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:31,504 : INFO : EPOCH 3 - PROGRESS: at 95.41% examples, 711107 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:32,169 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-24 12:41:32,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-24 12:41:32,187 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-24 12:41:32,193 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-24 12:41:32,195 : INFO : EPOCH - 3 : training on 17798082 raw words (12746584 effective words) took 17.8s, 716849 effective words/s\n",
      "2018-10-24 12:41:33,207 : INFO : EPOCH 4 - PROGRESS: at 6.51% examples, 825330 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:34,207 : INFO : EPOCH 4 - PROGRESS: at 12.74% examples, 805571 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:35,209 : INFO : EPOCH 4 - PROGRESS: at 18.36% examples, 772682 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:36,223 : INFO : EPOCH 4 - PROGRESS: at 24.27% examples, 764678 words/s, in_qsize 7, out_qsize 1\n",
      "2018-10-24 12:41:37,223 : INFO : EPOCH 4 - PROGRESS: at 30.09% examples, 760174 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:38,224 : INFO : EPOCH 4 - PROGRESS: at 35.33% examples, 743017 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:39,239 : INFO : EPOCH 4 - PROGRESS: at 41.14% examples, 741589 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:40,243 : INFO : EPOCH 4 - PROGRESS: at 46.94% examples, 741412 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:41,252 : INFO : EPOCH 4 - PROGRESS: at 52.60% examples, 738599 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:42,259 : INFO : EPOCH 4 - PROGRESS: at 57.73% examples, 730703 words/s, in_qsize 7, out_qsize 1\n",
      "2018-10-24 12:41:43,259 : INFO : EPOCH 4 - PROGRESS: at 64.05% examples, 737677 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:44,267 : INFO : EPOCH 4 - PROGRESS: at 70.33% examples, 742461 words/s, in_qsize 7, out_qsize 1\n",
      "2018-10-24 12:41:45,279 : INFO : EPOCH 4 - PROGRESS: at 76.98% examples, 750069 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:46,292 : INFO : EPOCH 4 - PROGRESS: at 83.39% examples, 753991 words/s, in_qsize 6, out_qsize 0\n",
      "2018-10-24 12:41:47,295 : INFO : EPOCH 4 - PROGRESS: at 87.80% examples, 741304 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:48,312 : INFO : EPOCH 4 - PROGRESS: at 93.12% examples, 736683 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:49,313 : INFO : EPOCH 4 - PROGRESS: at 98.95% examples, 737086 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:49,454 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-24 12:41:49,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-24 12:41:49,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-24 12:41:49,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-24 12:41:49,480 : INFO : EPOCH - 4 : training on 17798082 raw words (12748407 effective words) took 17.3s, 737793 effective words/s\n",
      "2018-10-24 12:41:50,491 : INFO : EPOCH 5 - PROGRESS: at 5.42% examples, 691330 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:51,498 : INFO : EPOCH 5 - PROGRESS: at 11.61% examples, 732585 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:52,502 : INFO : EPOCH 5 - PROGRESS: at 17.30% examples, 726071 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:53,509 : INFO : EPOCH 5 - PROGRESS: at 22.98% examples, 723841 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:54,510 : INFO : EPOCH 5 - PROGRESS: at 28.20% examples, 711982 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:55,550 : INFO : EPOCH 5 - PROGRESS: at 33.82% examples, 706942 words/s, in_qsize 6, out_qsize 2\n",
      "2018-10-24 12:41:56,548 : INFO : EPOCH 5 - PROGRESS: at 38.68% examples, 694721 words/s, in_qsize 6, out_qsize 0\n",
      "2018-10-24 12:41:57,553 : INFO : EPOCH 5 - PROGRESS: at 44.20% examples, 696015 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:41:58,561 : INFO : EPOCH 5 - PROGRESS: at 50.11% examples, 702274 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:41:59,562 : INFO : EPOCH 5 - PROGRESS: at 55.61% examples, 702009 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:42:00,579 : INFO : EPOCH 5 - PROGRESS: at 61.18% examples, 703123 words/s, in_qsize 5, out_qsize 1\n",
      "2018-10-24 12:42:01,578 : INFO : EPOCH 5 - PROGRESS: at 67.23% examples, 708544 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:42:02,581 : INFO : EPOCH 5 - PROGRESS: at 73.50% examples, 715502 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:42:03,581 : INFO : EPOCH 5 - PROGRESS: at 79.57% examples, 719514 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:42:04,607 : INFO : EPOCH 5 - PROGRESS: at 85.85% examples, 723680 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:42:05,612 : INFO : EPOCH 5 - PROGRESS: at 90.93% examples, 718877 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-24 12:42:06,613 : INFO : EPOCH 5 - PROGRESS: at 97.07% examples, 722363 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-24 12:42:07,134 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-24 12:42:07,144 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-24 12:42:07,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-24 12:42:07,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-24 12:42:07,158 : INFO : EPOCH - 5 : training on 17798082 raw words (12750308 effective words) took 17.7s, 721528 effective words/s\n",
      "2018-10-24 12:42:07,160 : INFO : training on a 88990410 raw words (63747696 effective words) took 84.1s, 758180 effective words/s\n",
      "2018-10-24 12:42:07,162 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8c32783bad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m                          \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_word_count\u001b[0m\u001b[0;34m,\u001b[0m                          \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"300features_40minwords_10context\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print \"Training model\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                         size=num_features, min_count=min_word_count, \\\n",
    "                         window = context, sample = downsampling)\n",
    "model.init_sims(replace=True)\n",
    "Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter+=1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im going to try bag of words to just make sure Im not crazy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "baggableReviews = []\n",
    "for review in clean_train_reviews:\n",
    "    baggableReviews.append(\" \".join(review))\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(baggableReviews)\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv == False:\n",
    "        return 1/(1+np.exp(-x))\n",
    "    else:\n",
    "        return x*(1-x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def relu(x, deriv=False):\n",
    "    if deriv:\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "    else:\n",
    "        return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0]\n",
    "import datetime\n",
    "import json\n",
    "def trainN(X, y, hidden_neurons=10, alpha=1, epochs=10000, dropout=False, dropout_percent=0.5, batchSize=100):\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, 1) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), 30)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "      \n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "  \n",
    "    for batchIndex in iter(range(X.shape[0]/batchSize - 90)):\n",
    "        bInputs = X[batchIndex*batchSize:(batchIndex+1)*batchSize]\n",
    "        bOutputs = y[batchIndex*batchSize:(batchIndex+1)*batchSize]\n",
    "        \n",
    "        last_mean_error = 1\n",
    "        \n",
    "        print \"Starting Batch \" + str(batchIndex+1)\n",
    "        for j in iter(range(epochs+1)):\n",
    "            layer_0 = bInputs\n",
    "            layer_1 = relu(np.dot(layer_0, synapse_0))\n",
    "            if(dropout):\n",
    "                layer_1 *= np.random.binomials([np.ones((len(X), hidden_neurons))], 1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "            layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "            layer_2_error = bOutputs - layer_2\n",
    "\n",
    "            if (j% 1000) == 0 and j > 5000:\n",
    "                if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                    print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                    last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "                else:\n",
    "                    print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                    break\n",
    "\n",
    "            layer_2_delta = layer_2_error * sigmoid(layer_2,True)\n",
    "\n",
    "            layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "            layer_1_delta = layer_1_error * relu(layer_1,True)\n",
    "\n",
    "            synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "            synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "            \n",
    "\n",
    "            if (j > 0):\n",
    "                synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "                synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))\n",
    "\n",
    "            synapse_1 += alpha * synapse_1_weight_update\n",
    "            synapse_0 += alpha * synapse_0_weight_update\n",
    "\n",
    "            prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "            prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "        print \"Batch Error:\"\n",
    "        print layer_2_error.mean()\n",
    "        \n",
    "#             print \"Current Epoch: \" + str(j)\n",
    "#             print layer_2_error[0:10]\n",
    "            \n",
    "\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "             'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "             'classes': [0,1]\n",
    "            }\n",
    "    synapse_file = \"synapses.json\"\n",
    "#   Doesn't work in collab\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)\n",
    "  \n",
    "    return synapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape[0]/250 -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00416963,  0.00545215, -0.00675965, ..., -0.01853286,\n",
       "        -0.00119325,  0.00321041],\n",
       "       [ 0.01587193, -0.0038891 ,  0.01554895, ..., -0.02180508,\n",
       "        -0.03443483, -0.0131663 ],\n",
       "       [-0.00130455,  0.01269539, -0.02064335, ...,  0.02835437,\n",
       "        -0.00968054,  0.00894158],\n",
       "       ...,\n",
       "       [-0.01082018, -0.00066697,  0.01261383, ..., -0.01127248,\n",
       "         0.00220335, -0.02739951],\n",
       "       [ 0.0193611 ,  0.0189328 , -0.03613501, ..., -0.03127817,\n",
       "        -0.0182154 , -0.01936423],\n",
       "       [-0.01111796,  0.00228005,  0.000993  , ..., -0.00814698,\n",
       "        -0.02333804, -0.00455689]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainOutputs = np.array(train[\"sentiment\"]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2x1 = []\n",
    "for output in trainOutputs:\n",
    "    if output == 1:\n",
    "        outputs2x1.append([0,1])\n",
    "    else: \n",
    "        outputs2x1.append([1,0])\n",
    "outputs2x1 = np.array(outputs2x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 30 neurons, alpha:1, dropout:False \n",
      "Input matrix: 18800x300    Output matrix: 1x1\n",
      "Starting Batch 1\n",
      "delta after 6000 iterations:0.002491857524616934\n",
      "delta after 7000 iterations:0.0023175673548174367\n",
      "delta after 8000 iterations:0.002176255809572397\n",
      "delta after 9000 iterations:0.0020573951903208503\n",
      "delta after 10000 iterations:0.001956010268206806\n",
      "Batch Error:\n",
      "-1.886966250713655e-05\n",
      "Starting Batch 2\n",
      "delta after 6000 iterations:0.024350535567227126\n",
      "delta after 7000 iterations:0.02429294518494054\n",
      "delta after 8000 iterations:0.0242459144639076\n",
      "delta after 9000 iterations:0.024206799639773166\n",
      "delta after 10000 iterations:0.024174048868893384\n",
      "Batch Error:\n",
      "-0.02351218146752443\n",
      "Starting Batch 3\n",
      "delta after 6000 iterations:0.03257260027379509\n",
      "delta after 7000 iterations:0.03237806467630906\n",
      "('break:', 0.032435757998973205, '>', 0.03237806467630906)\n",
      "Batch Error:\n",
      "-0.008049288005431799\n",
      "Starting Batch 4\n",
      "delta after 6000 iterations:0.0009266335597701937\n",
      "delta after 7000 iterations:0.0008599793208400439\n",
      "delta after 8000 iterations:0.0008073020185950981\n",
      "delta after 9000 iterations:0.0007645723192321431\n",
      "delta after 10000 iterations:0.0007288553152711212\n",
      "Batch Error:\n",
      "3.199910167260152e-05\n",
      "Starting Batch 5\n",
      "delta after 6000 iterations:0.000741090278884339\n",
      "delta after 7000 iterations:0.0006927983102109279\n",
      "delta after 8000 iterations:0.000654045929193578\n",
      "delta after 9000 iterations:0.0006215207144488348\n",
      "delta after 10000 iterations:0.0005936700305608304\n",
      "Batch Error:\n",
      "5.483841038695219e-05\n",
      "Starting Batch 6\n",
      "delta after 6000 iterations:0.06836157710558227\n",
      "delta after 7000 iterations:0.06767580994822958\n",
      "delta after 8000 iterations:0.06750364574166087\n",
      "delta after 9000 iterations:0.06743886178988136\n",
      "delta after 10000 iterations:0.034113217941512244\n",
      "Batch Error:\n",
      "-0.023811815254435417\n",
      "Starting Batch 7\n",
      "delta after 6000 iterations:0.032070188478146945\n",
      "delta after 7000 iterations:0.032016149988179074\n",
      "delta after 8000 iterations:0.031971932758234116\n",
      "delta after 9000 iterations:0.03193498247194005\n",
      "delta after 10000 iterations:0.031903352305743406\n",
      "Batch Error:\n",
      "0.01551450271212655\n",
      "Starting Batch 8\n",
      "delta after 6000 iterations:0.008414239183486483\n",
      "delta after 7000 iterations:0.008376496097433881\n",
      "delta after 8000 iterations:0.008345845008308511\n",
      "delta after 9000 iterations:0.008320251203532565\n",
      "delta after 10000 iterations:0.008298421104345175\n",
      "Batch Error:\n",
      "-0.007883527764461834\n",
      "Starting Batch 9\n",
      "delta after 6000 iterations:0.03175920048484916\n",
      "delta after 7000 iterations:0.03172931898298116\n",
      "delta after 8000 iterations:0.03170469714750478\n",
      "delta after 9000 iterations:0.031683920603430574\n",
      "delta after 10000 iterations:0.03166601462980217\n",
      "Batch Error:\n",
      "-0.03124532376488032\n",
      "Starting Batch 10\n",
      "delta after 6000 iterations:0.024347480967998464\n",
      "delta after 7000 iterations:0.024288789177969682\n",
      "delta after 8000 iterations:0.024240719459885058\n",
      "delta after 9000 iterations:0.024200454051806635\n",
      "delta after 10000 iterations:0.024166032164623888\n",
      "Batch Error:\n",
      "-0.007840771924144632\n",
      "Starting Batch 11\n",
      "delta after 6000 iterations:0.03218178571013225\n",
      "delta after 7000 iterations:0.03213037700860834\n",
      "delta after 8000 iterations:0.03209355230404695\n",
      "('break:', 0.03213152238696641, '>', 0.03209355230404695)\n",
      "Batch Error:\n",
      "-0.00024155956822497025\n",
      "Starting Batch 12\n",
      "delta after 6000 iterations:0.016135532367202558\n",
      "delta after 7000 iterations:0.01606986425437596\n",
      "delta after 8000 iterations:0.016030265663383253\n",
      "delta after 9000 iterations:0.01600188684275064\n",
      "delta after 10000 iterations:0.01598014600172719\n",
      "Batch Error:\n",
      "2.320069688039628e-05\n",
      "Starting Batch 13\n",
      "delta after 6000 iterations:0.0006064273191118245\n",
      "delta after 7000 iterations:0.0005662393653162569\n",
      "delta after 8000 iterations:0.0005334929414773723\n",
      "delta after 9000 iterations:0.000505896104082066\n",
      "delta after 10000 iterations:0.0004822842545201914\n",
      "Batch Error:\n",
      "-2.09905601233356e-05\n",
      "Starting Batch 14\n",
      "delta after 6000 iterations:0.031953253775292796\n",
      "delta after 7000 iterations:0.031908096780835074\n",
      "delta after 8000 iterations:0.03187131422278207\n",
      "delta after 9000 iterations:0.03184068044808852\n",
      "delta after 10000 iterations:0.031814811266299314\n",
      "Batch Error:\n",
      "-2.0682232827121233e-05\n",
      "Starting Batch 15\n",
      "delta after 6000 iterations:0.02469134553172854\n",
      "delta after 7000 iterations:0.024546532394574244\n",
      "delta after 8000 iterations:0.024448465863096534\n",
      "delta after 9000 iterations:0.024375247873026494\n",
      "delta after 10000 iterations:0.02431893633122298\n",
      "Batch Error:\n",
      "-0.007819185452387164\n",
      "Starting Batch 16\n",
      "delta after 6000 iterations:0.03969634728119444\n",
      "delta after 7000 iterations:0.039664737812923255\n",
      "delta after 8000 iterations:0.039639286424304324\n",
      "delta after 9000 iterations:0.039618864176671445\n",
      "delta after 10000 iterations:0.03960355300470229\n",
      "Batch Error:\n",
      "-0.007841632293745066\n",
      "Starting Batch 17\n",
      "delta after 6000 iterations:0.016398091601463688\n",
      "delta after 7000 iterations:0.01625035306817398\n",
      "delta after 8000 iterations:0.01617351624662072\n",
      "delta after 9000 iterations:0.016126104714398827\n",
      "delta after 10000 iterations:0.016090998959841875\n",
      "Batch Error:\n",
      "-1.0921774968082436e-05\n",
      "Starting Batch 18\n",
      "delta after 6000 iterations:0.02393171841126159\n",
      "delta after 7000 iterations:0.023912806327029162\n",
      "delta after 8000 iterations:0.016682140253800583\n",
      "delta after 9000 iterations:0.016391940180233936\n",
      "delta after 10000 iterations:0.0162853289562779\n",
      "Batch Error:\n",
      "-0.00013170244449834398\n",
      "Starting Batch 19\n",
      "delta after 6000 iterations:0.02022105782861443\n",
      "delta after 7000 iterations:0.02017511516049524\n",
      "delta after 8000 iterations:0.020137918491049293\n",
      "delta after 9000 iterations:0.01643402619218044\n",
      "delta after 10000 iterations:0.01626491451063924\n",
      "Batch Error:\n",
      "0.015604956367358534\n",
      "Starting Batch 20\n",
      "delta after 6000 iterations:0.0399268114529414\n",
      "delta after 7000 iterations:0.03213463204919412\n",
      "delta after 8000 iterations:0.031973402249038566\n",
      "delta after 9000 iterations:0.03189487527427592\n",
      "delta after 10000 iterations:0.031850681252931244\n",
      "Batch Error:\n",
      "-0.0001020940413574865\n",
      "Starting Batch 21\n",
      "delta after 6000 iterations:0.023924510173098244\n",
      "delta after 7000 iterations:0.02386476298936337\n",
      "delta after 8000 iterations:0.023826678901495996\n",
      "delta after 9000 iterations:0.02380055212531205\n",
      "delta after 10000 iterations:0.023780402526278333\n",
      "Batch Error:\n",
      "-0.023456548446772797\n",
      "Starting Batch 22\n",
      "delta after 6000 iterations:0.03602552424595572\n",
      "delta after 7000 iterations:0.035749735788334826\n",
      "delta after 8000 iterations:0.035665452847749184\n",
      "delta after 9000 iterations:0.035615079269241325\n",
      "delta after 10000 iterations:0.03557983116894876\n",
      "Batch Error:\n",
      "-0.02731305982629883\n",
      "Starting Batch 23\n",
      "delta after 6000 iterations:0.0863480085701982\n",
      "delta after 7000 iterations:0.08632011786257975\n",
      "delta after 8000 iterations:0.0862974675477291\n",
      "delta after 9000 iterations:0.08627862336901924\n",
      "delta after 10000 iterations:0.08626260655531154\n",
      "Batch Error:\n",
      "-0.03904315754197069\n",
      "Starting Batch 24\n",
      "delta after 6000 iterations:0.07840995277780582\n",
      "delta after 7000 iterations:0.07839442476796156\n",
      "delta after 8000 iterations:0.07838144323141857\n",
      "delta after 9000 iterations:0.07837062853848978\n",
      "delta after 10000 iterations:0.07836422248356176\n",
      "Batch Error:\n",
      "-1.2534279408686101e-05\n",
      "Starting Batch 25\n",
      "delta after 6000 iterations:0.039198277637047634\n",
      "delta after 7000 iterations:0.03919156576640128\n",
      "delta after 8000 iterations:0.0391861678300232\n",
      "delta after 9000 iterations:0.0391817283667498\n",
      "delta after 10000 iterations:0.03917802083978601\n",
      "Batch Error:\n",
      "0.023424943598461194\n",
      "Starting Batch 26\n",
      "delta after 6000 iterations:0.03976669046665551\n",
      "('break:', 0.03977767625654005, '>', 0.03976669046665551)\n",
      "Batch Error:\n",
      "0.023601673142868478\n",
      "Starting Batch 27\n",
      "delta after 6000 iterations:0.0630517652230272\n",
      "delta after 7000 iterations:0.05552394380744052\n",
      "delta after 8000 iterations:0.05531092150197692\n",
      "delta after 9000 iterations:0.055222093968079144\n",
      "delta after 10000 iterations:0.05516888930096504\n",
      "Batch Error:\n",
      "-0.023389528243114514\n",
      "Starting Batch 28\n",
      "delta after 6000 iterations:0.039637023410910605\n",
      "delta after 7000 iterations:0.03208210038648881\n",
      "delta after 8000 iterations:0.02387324130014405\n",
      "delta after 9000 iterations:0.023817340616935672\n",
      "delta after 10000 iterations:0.023783240144354994\n",
      "Batch Error:\n",
      "0.007826099306624753\n",
      "Starting Batch 29\n",
      "delta after 6000 iterations:0.03994938212730477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta after 7000 iterations:0.03987040421169788\n",
      "delta after 8000 iterations:0.03981043258423682\n",
      "delta after 9000 iterations:0.03976316937400566\n",
      "delta after 10000 iterations:0.03972439277430498\n",
      "Batch Error:\n",
      "0.007800677277541126\n",
      "Starting Batch 30\n",
      "delta after 6000 iterations:0.02395573577800217\n",
      "delta after 7000 iterations:0.02392237491193449\n",
      "delta after 8000 iterations:0.023895451165654467\n",
      "delta after 9000 iterations:0.02387322714447908\n",
      "delta after 10000 iterations:0.023854245073525213\n",
      "Batch Error:\n",
      "-0.023406645540176605\n",
      "Starting Batch 31\n",
      "delta after 6000 iterations:0.0783460524023325\n",
      "delta after 7000 iterations:0.07834324502517487\n",
      "('break:', 0.07834809947772631, '>', 0.07834324502517487)\n",
      "Batch Error:\n",
      "-0.01572529593974872\n",
      "Starting Batch 32\n",
      "delta after 6000 iterations:0.10178225851074008\n",
      "delta after 7000 iterations:0.10177004847356975\n",
      "delta after 8000 iterations:0.10176015505646192\n",
      "delta after 9000 iterations:0.10175226200106477\n",
      "delta after 10000 iterations:0.10174590402394638\n",
      "Batch Error:\n",
      "0.023460162028528724\n",
      "Starting Batch 33\n",
      "delta after 6000 iterations:0.023651054556316248\n",
      "delta after 7000 iterations:0.023638612153291327\n",
      "delta after 8000 iterations:0.023628481488584718\n",
      "delta after 9000 iterations:0.023620006883624492\n",
      "delta after 10000 iterations:0.02361276795425213\n",
      "Batch Error:\n",
      "-0.00786481280574447\n",
      "Starting Batch 34\n",
      "delta after 6000 iterations:0.05505023704949642\n",
      "delta after 7000 iterations:0.05502627046154623\n",
      "delta after 8000 iterations:0.055007097953343576\n",
      "delta after 9000 iterations:0.05499134433840078\n",
      "delta after 10000 iterations:0.05497924158125228\n",
      "Batch Error:\n",
      "0.039067192187562334\n",
      "Starting Batch 35\n",
      "delta after 6000 iterations:0.07061368695318812\n",
      "delta after 7000 iterations:0.07059383619755387\n",
      "delta after 8000 iterations:0.07057858106328511\n",
      "delta after 9000 iterations:0.07056657209940108\n",
      "delta after 10000 iterations:0.07055683784354302\n",
      "Batch Error:\n",
      "-0.007845154858971776\n",
      "Starting Batch 36\n",
      "delta after 6000 iterations:0.015852223709463827\n",
      "delta after 7000 iterations:0.01584216885959424\n",
      "delta after 8000 iterations:0.015833737890059506\n",
      "delta after 9000 iterations:0.015826517433624114\n",
      "delta after 10000 iterations:0.015820228895873593\n",
      "Batch Error:\n",
      "-0.015679175360834872\n",
      "Starting Batch 37\n",
      "delta after 6000 iterations:0.07837794938283565\n",
      "delta after 7000 iterations:0.07835938629734954\n",
      "delta after 8000 iterations:0.07834478268424178\n",
      "delta after 9000 iterations:0.07833276832738718\n",
      "delta after 10000 iterations:0.07832393880793442\n",
      "Batch Error:\n",
      "0.046822735451032604\n",
      "Starting Batch 38\n",
      "delta after 6000 iterations:0.0161138475793638\n",
      "delta after 7000 iterations:0.016083110969646083\n",
      "delta after 8000 iterations:0.016058565721888667\n",
      "delta after 9000 iterations:0.016038042723486773\n",
      "delta after 10000 iterations:0.01602046296449012\n",
      "Batch Error:\n",
      "5.781417721433674e-06\n",
      "Starting Batch 39\n",
      "delta after 6000 iterations:0.02368617979002143\n",
      "delta after 7000 iterations:0.023675230914407798\n",
      "delta after 8000 iterations:0.023667229950778045\n",
      "delta after 9000 iterations:0.023666268427714666\n",
      "delta after 10000 iterations:0.016016280412950973\n",
      "Batch Error:\n",
      "-0.0001518670554613074\n",
      "Starting Batch 40\n",
      "delta after 6000 iterations:0.039419500487537035\n",
      "delta after 7000 iterations:0.03939342780013325\n",
      "delta after 8000 iterations:0.03937303417601243\n",
      "delta after 9000 iterations:0.03935634956822519\n",
      "delta after 10000 iterations:0.03934231639003076\n",
      "Batch Error:\n",
      "-0.02350823960595709\n",
      "Starting Batch 41\n",
      "delta after 6000 iterations:0.0316630755313342\n",
      "delta after 7000 iterations:0.023854758310934365\n",
      "delta after 8000 iterations:0.023768610061216\n",
      "delta after 9000 iterations:0.023728709541679785\n",
      "delta after 10000 iterations:0.023702827383933124\n",
      "Batch Error:\n",
      "-0.0077982754914895204\n",
      "Starting Batch 42\n",
      "delta after 6000 iterations:0.06302068116740459\n",
      "delta after 7000 iterations:0.06296377454335234\n",
      "delta after 8000 iterations:0.06293294758617568\n",
      "('break:', 0.06293614625961559, '>', 0.06293294758617568)\n",
      "Batch Error:\n",
      "-0.03111373826040652\n",
      "Starting Batch 43\n",
      "delta after 6000 iterations:0.047518192884621537\n",
      "delta after 7000 iterations:0.04746694825760347\n",
      "delta after 8000 iterations:0.04742700598158664\n",
      "delta after 9000 iterations:0.047395624948568735\n",
      "delta after 10000 iterations:0.03977904454433186\n",
      "Batch Error:\n",
      "-0.00796930477450905\n",
      "Starting Batch 44\n",
      "delta after 6000 iterations:0.03204656076742151\n",
      "delta after 7000 iterations:0.02446394887201401\n",
      "delta after 8000 iterations:0.024263551869467256\n",
      "delta after 9000 iterations:0.02417829887701402\n",
      "delta after 10000 iterations:0.024122745910812902\n",
      "Batch Error:\n",
      "0.007801484719587184\n",
      "Starting Batch 45\n",
      "delta after 6000 iterations:0.06279808280285999\n",
      "delta after 7000 iterations:0.06277983775066355\n",
      "delta after 8000 iterations:0.06276510987739684\n",
      "delta after 9000 iterations:0.06275306598139914\n",
      "delta after 10000 iterations:0.06274319343527773\n",
      "Batch Error:\n",
      "-0.015573779780022143\n",
      "Starting Batch 46\n",
      "delta after 6000 iterations:0.039371250433969364\n",
      "delta after 7000 iterations:0.03933775115955164\n",
      "delta after 8000 iterations:0.03931429827851991\n",
      "delta after 9000 iterations:0.03929667190333816\n",
      "delta after 10000 iterations:0.039282657732798626\n",
      "Batch Error:\n",
      "0.007761387757517818\n",
      "Starting Batch 47\n",
      "delta after 6000 iterations:0.0940009874125474\n",
      "delta after 7000 iterations:0.09398389915718308\n",
      "delta after 8000 iterations:0.09397014096081557\n",
      "delta after 9000 iterations:0.09395989165998625\n",
      "delta after 10000 iterations:0.09395691940273584\n",
      "Batch Error:\n",
      "-6.568903173185744e-05\n",
      "Starting Batch 48\n",
      "delta after 6000 iterations:0.03172397879828785\n",
      "delta after 7000 iterations:0.03168939045549073\n",
      "delta after 8000 iterations:0.031662488711932536\n",
      "delta after 9000 iterations:0.03164052847200826\n",
      "delta after 10000 iterations:0.03162207631184373\n",
      "Batch Error:\n",
      "0.015600796349190222\n",
      "Starting Batch 49\n",
      "delta after 6000 iterations:0.06292585520601862\n",
      "delta after 7000 iterations:0.06289989123700264\n",
      "delta after 8000 iterations:0.06287862988540696\n",
      "delta after 9000 iterations:0.06286065068623511\n",
      "delta after 10000 iterations:0.06284516865566052\n",
      "Batch Error:\n",
      "0.01562786176806638\n",
      "Starting Batch 50\n",
      "delta after 6000 iterations:0.05509086389343204\n",
      "delta after 7000 iterations:0.05505882548837221\n",
      "delta after 8000 iterations:0.05503470219149694\n",
      "delta after 9000 iterations:0.05501583980103532\n",
      "delta after 10000 iterations:0.05499946723903905\n",
      "Batch Error:\n",
      "-0.007804304594935526\n",
      "Starting Batch 51\n",
      "delta after 6000 iterations:0.05527153962943973\n",
      "delta after 7000 iterations:0.05522325175792985\n",
      "delta after 8000 iterations:0.0551887636588783\n",
      "delta after 9000 iterations:0.05516113186991303\n",
      "delta after 10000 iterations:0.055138628835689346\n",
      "Batch Error:\n",
      "-0.0077657260068809564\n",
      "Starting Batch 52\n",
      "delta after 6000 iterations:0.04797253940885498\n",
      "delta after 7000 iterations:0.04746086256756962\n",
      "delta after 8000 iterations:0.04736293212073089\n",
      "delta after 9000 iterations:0.047313006211668576\n",
      "delta after 10000 iterations:0.041374700241127965\n",
      "Batch Error:\n",
      "-0.022977082585947658\n",
      "Starting Batch 53\n",
      "delta after 6000 iterations:0.047242744382599455\n",
      "delta after 7000 iterations:0.047218689694127694\n",
      "delta after 8000 iterations:0.047216451410085805\n",
      "delta after 9000 iterations:0.0397635156400529\n",
      "delta after 10000 iterations:0.03956430849725385\n",
      "Batch Error:\n",
      "-0.007863283284149565\n",
      "Starting Batch 54\n",
      "delta after 6000 iterations:0.047201351201587166\n",
      "delta after 7000 iterations:0.047185419140553825\n",
      "delta after 8000 iterations:0.047177291866094664\n",
      "('break:', 0.04721220314032531, '>', 0.047177291866094664)\n",
      "Batch Error:\n",
      "-0.015488779746085157\n",
      "Starting Batch 55\n",
      "delta after 6000 iterations:0.0626944591229484\n",
      "delta after 7000 iterations:0.06268269003276047\n",
      "delta after 8000 iterations:0.06267327949534757\n",
      "delta after 9000 iterations:0.0626656798360217\n",
      "delta after 10000 iterations:0.06265922559704426\n",
      "Batch Error:\n",
      "-3.0674719374030462e-06\n",
      "Starting Batch 56\n",
      "delta after 6000 iterations:0.05482493246318853\n",
      "delta after 7000 iterations:0.054822546567792896\n",
      "delta after 8000 iterations:0.047176586258797275\n",
      "delta after 9000 iterations:0.04706706563540793\n",
      "delta after 10000 iterations:0.04703467322886947\n",
      "Batch Error:\n",
      "-0.015646161424585654\n",
      "('saved synapses to:', 'synapses.json')\n"
     ]
    }
   ],
   "source": [
    "synapses = trainN(X_train, y_train, hidden_neurons=30, alpha=1, epochs=10000, dropout=False, dropout_percent=0.5, batchSize=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def think(data, synapses, needProcessing=False):\n",
    "    synapse_0 = synapses[\"synapse0\"]\n",
    "    synapse_1 = synapses[\"synapse1\"]\n",
    "    if needProcessing:\n",
    "        processedData = getAvgFeatureVecs( review_to_wordlist(data), model, num_features )\n",
    "        l1 = relu(np.dot(processedData, synapse_0))\n",
    "    else:\n",
    "        l1 = relu(np.dot(data, synapse_0))\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainDataVecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c301906a3c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mERROR_THRESHOLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mholdouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m95\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m95\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mholdoutAK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs2x1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m95\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m95\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# first column is 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Second column is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainDataVecs' is not defined"
     ]
    }
   ],
   "source": [
    "ERROR_THRESHOLD = 0.2\n",
    "holdouts = trainDataVecs[95*250:(95+1)*250]\n",
    "holdoutAK = outputs2x1[95*250:(95+1)*250]\n",
    "# first column is 0\n",
    "# Second column is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in maximum\n"
     ]
    }
   ],
   "source": [
    "outputs = think(X_test, synapses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorT = (y_test-outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.029577492436887908\n"
     ]
    }
   ],
   "source": [
    "eliminatedNan = []\n",
    "for e in errorT:\n",
    "    if np.isnan(e) == False:\n",
    "        eliminatedNan.append(e)\n",
    "        \n",
    "print np.array(eliminatedNan).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "roundedOutputs = outputs.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roundError = y_test-roundedOutputs\n",
    "roundError[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = np.array([x[0] for x in outputs])\n",
    "col2 = np.array([x[1] for x in outputs])\n",
    "ccol1 = np.array([x[0] for x in holdoutAK])\n",
    "ccol2 = np.array([x[1] for x in holdoutAK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "error0 = col1-ccol1\n",
    "error1 = col2-ccol2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
